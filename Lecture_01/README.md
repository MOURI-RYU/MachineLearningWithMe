### 1. 本节视频6,7
### 2. 思考问题:
1. 为什么说线性回归中误差是服从均值为0的方差为sigma^2的正态(高斯)分布，不是0均值行不行？<br>

2. 什么是最小二乘法？<br>
3. 为什么要用最小二乘法而不是最小四乘法，六乘法？<br>
4. 怎么理解似然函数(likelihood function)<br>
5. 怎么理解梯度与学习率（Gradient and learning rate）？<br>
6. 怎么理解梯度下降算法（Gradient Descent）？<br>
7. 运用梯度下降算法的前提是什么？<br>
8. 梯度下降算法是否一定能找到最优解？<br>
9. 学习率过大或者过小将会对目标函数产生什么样的影响？<br>
10. 什么是feature scalling<br>

参考 [线性回归](https://blog.csdn.net/The_lastest/article/details/82556307)
### 3. 算法示例:
- 示例1[波士顿房价预测](LinearRegression.py)<br>
涉及知识点：
    1. 数据归一化(feature scalling)
    2. 数据打乱(shuffle)
    3. 实现梯度下降
<br>
<br>
 ### [<主页>](../README.md) [<下一讲>](../Lecture_02/README.md)